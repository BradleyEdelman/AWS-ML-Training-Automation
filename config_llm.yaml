model: gpt2
tokenizer: gpt2
max_length: 512
batch_size: 8
learning_rate: 0.00005
weight_decay: 0.01
num_layers_to_unfreeze: 5