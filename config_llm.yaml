model: "gpt2"
tokenizer: "gpt2"
data_filename: 'training_data.json'
max_length: 512
batch_size: 8
learning_rate: 0.00005
weight_decay: 0.01
num_layers_to_unfreeze: 5
gradient_accumulation_steps: 4
use_fp16: true
